{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6hOAN5twj64k"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "\n",
        "loader = PyPDFLoader(\"data\\ConceptsofBiology-WEB.pdf\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pages 19 to 76 are the two chapters\n",
        "data = data[18:36]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gYYCuz9Qj64l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
        "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "naku1wvej64l",
        "outputId": "de9c3437-bb95-43ab-922d-a3ad75225142"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Quantiphi-Assignment\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the path to the pre-trained model you want to use\n",
        "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# modelPath = \"google/flan-t5-large\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8j5Se7JZj64l",
        "outputId": "59dc287b-44df-4b8f-c794-52f208bb689f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.02040591835975647, -0.033092256635427475, 0.003897689515724778]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"This is a test substance.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "query_result[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g_Q4GL7Ij64m"
      },
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(docs, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0BArdrFFj64m"
      },
      "outputs": [],
      "source": [
        "# Specify the model name you want to use\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "# Load the tokenizer associated with the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Define a question-answering pipeline using the model and tokenizer\n",
        "question_answerer = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer\n",
        "    , max_length=512\n",
        "    \n",
        "    # ,return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
        "# with additional model-specific arguments (temperature and max_length)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "W1WMwGRSj64m"
      },
      "outputs": [],
      "source": [
        "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
        "# This retriever is likely used for retrieving data or documents from the database.\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AInAdGBLj64m",
        "outputId": "967e7f1a-5858-4e39-882d-db08217f98d3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "i_NmQMY-j64m"
      },
      "outputs": [],
      "source": [
        "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
        "# It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "v8SPkusZj64m",
        "outputId": "849d5eab-7b2f-4d20-d6ae-32b261313bbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1643 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query:  what is Biology?\n",
            "Answer:  a broad br anch itself\n",
            "Query:  What are the properties of life? List them\n",
            "Answer:  All groups o f living or ganisms shar e se veral key char acteristics or functions: or der, sensitivity or response t o stimuli, r eproduction, adap tation, gr owth and de velopment, r egulation/homeos tasis, ener gy processing, and e volution.\n",
            "Query:  What is chemotaxis?\n",
            "Answer:  chemical\n",
            "Query:  What is adaptation? \n",
            "Answer:  All living or ganisms e xhibit a “ fit” to their en vironment.\n",
            "Query:  What does genes provide? Explain\n",
            "Answer:  Ins tructions nec essary for lif e.\n",
            "Query:  What is an atom?\n",
            "Answer:  The atomis the smal lest and mos t fundamental unit o f mat ter that r etains the pr oper ties o f an element.\n",
            "Query:  What is an organ system?\n",
            "Answer:  collections o f tissues gr ouped t ogether based on a c ommon function\n"
          ]
        }
      ],
      "source": [
        "questions = [\"what is Biology?\",\n",
        "             \"What are the properties of life? List them\",\n",
        "             \"What is chemotaxis?\",\n",
        "             \"What is adaptation? \",\n",
        "             \"What does genes provide? Explain\",\n",
        "             \"What is an atom?\",\n",
        "             \"What is an organ system?\"]\n",
        "for el in questions:\n",
        "    result = qa.invoke({'query': el})\n",
        "    print(\"Query: \",result['query'])\n",
        "    print(\"Answer: \",  result['result'])\n",
        "\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
