{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6hOAN5twj64k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Quantiphi-Assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data in a dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(r\"data\\ConceptsofBiology-WEB.pdf\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Select the first two chapters for speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pages 19 to 76 are the two chapters\n",
        "data = data[18:76]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the data and do chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "gYYCuz9Qj64l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
        "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
        "docs = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "naku1wvej64l",
        "outputId": "de9c3437-bb95-43ab-922d-a3ad75225142"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Quantiphi-Assignment\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the path to the pre-trained model you want to use\n",
        "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# modelPath = \"google/flan-t5-large\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ChromaDB (persistent) database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import chromadb\n",
        "# import os\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# def load_chunk_persist_pdf(modelPath = \"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs = {'device':'cpu'},\\\n",
        "#     encode_kwargs = {'normalize_embeddings': False}, pdf_folder_path= \"data\") -> Chroma:\n",
        "#     pdf_folder_path = pdf_folder_path\n",
        "#     documents = []\n",
        "#     for file in os.listdir(pdf_folder_path):\n",
        "#         if file.endswith('.pdf'):\n",
        "#             pdf_path = os.path.join(pdf_folder_path, file)\n",
        "#             loader = PyPDFLoader(pdf_path)\n",
        "#             documents.extend(loader.load())\n",
        "#     documents = documents[16:38]\n",
        "#     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "#     chunked_documents = text_splitter.split_documents(documents)\n",
        "#     client = chromadb.Client()\n",
        "#     if client.list_collections():\n",
        "#         consent_collection = client.create_collection(\"consent_collection\")\n",
        "#     else:\n",
        "#         print(\"Collection already exists\")\n",
        "#     vectordb = Chroma.from_documents(\n",
        "#         documents=chunked_documents,\n",
        "#         embedding=HuggingFaceEmbeddings(\n",
        "#     model_name=modelPath,     # Provide the pre-trained model's path\n",
        "#     model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "#     encode_kwargs=encode_kwargs # Pass the encoding options\n",
        "# ),\n",
        "#         persist_directory=\"chroma_store\"\n",
        "#     )\n",
        "#     vectordb.persist()\n",
        "#     return vectordb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Vector DB, FAISS (in-memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "g_Q4GL7Ij64m"
      },
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(docs, embeddings)\n",
        "# vectordb = load_chunk_persist_pdf()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create LLM instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0BArdrFFj64m"
      },
      "outputs": [],
      "source": [
        "# Specify the model name you want to use\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "# Load the tokenizer associated with the specified model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Define a question-answering pipeline using the model and tokenizer\n",
        "question_answerer = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer\n",
        "    , max_length=512\n",
        "    \n",
        "    # ,return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
        "# with additional model-specific arguments (temperature and max_length)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0.4, \"max_length\": 512}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create retreiver for question-answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "i_NmQMY-j64m"
      },
      "outputs": [],
      "source": [
        "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
        "# It's configured with a language model (llm), a chain type \"stuff,\" the retriever we created, and an option to not return source documents.\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# retriever = vectordb.as_retriever(search_kwargs={\"k\": 1})\n",
        "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "# def get_llm_response(query,qa):\n",
        "#     # matching_docs = vectordb.similarity_search(query)\n",
        "#     answer = qa.invoke({'query': query})\n",
        "#     return answer\n",
        "\n",
        "# questions = [\"what is Biology? \",\n",
        "#              \"What are the properties of life? List them\",\n",
        "#              \"What is chemotaxis?\",\n",
        "#              \"What is adaptation in Biology? \",\n",
        "#              \"What does genes provide? \",\n",
        "#              \"What is an atom?\",\n",
        "#              \"What is an organ system?\"]\n",
        "# for el in questions:\n",
        "#     response = get_llm_response(el,qa)\n",
        "#     print(\"Query: \",el)\n",
        "#     print(\"Answer: \", response['result'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the model with sample questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "v8SPkusZj64m",
        "outputId": "849d5eab-7b2f-4d20-d6ae-32b261313bbe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query:  what is Biology?\n",
            "Answer:  Scienc e of life\n",
            "Query:  What are the properties of life? List them\n",
            "Answer:  All groups o f living or ganisms shar e se veral key char acteristics or functions: or der, sensitivity or response t o stimuli, r eproduction, adap tation, gr owth and de velopment, r egulation/homeos tasis, ener gy processing, and e volution.\n",
            "Query:  What is chemotaxis?\n",
            "Answer:  bact eria can mo ve toward or a way from chemicals (a process cal led chemotaxis) or light (phot otaxis).\n",
            "Query:  What is adaptation? \n",
            "Answer:  All living or ganisms e xhibit a “ fit” to their en vironment.\n",
            "Query:  What does genes provide? Explain\n",
            "Answer:  Ins tructions nec essary for lif e.\n",
            "Query:  What is an atom?\n",
            "Answer:  smal lest component o f an element that r etains al l of the chemical pr oper ties o f that element.\n",
            "Query:  What is an organ system?\n",
            "Answer:  collections o f tissues gr ouped t ogether based on a c ommon function\n"
          ]
        }
      ],
      "source": [
        "questions = [\"what is Biology?\",\n",
        "             \"What are the properties of life? List them\",\n",
        "             \"What is chemotaxis?\",\n",
        "             \"What is adaptation? \",\n",
        "             \"What does genes provide? Explain\",\n",
        "             \"What is an atom?\",\n",
        "             \"What is an organ system?\"]\n",
        "for el in questions:\n",
        "    result = qa.invoke({'query': el})\n",
        "    print(\"Query: \",result['query'])\n",
        "    print(\"Answer: \",  result['result'])\n",
        "\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
